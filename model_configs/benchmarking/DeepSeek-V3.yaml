ENV_VARS:
  CUDA_DEVICE_MAX_CONNECTIONS: 32 # TODO: set to 1 if not using 1F1B overlapping
  TORCH_NCCL_AVOID_RECORD_STREAMS: 0
  NVTE_ALLOW_NONDETERMINISTIC_ALGO: 1
  PYTORCH_CUDA_ALLOC_CONF: expandable_segments:True
  NCCL_NVLS_ENABLE: 0
  NVTE_FUSED_ATTN: 1
  NVTE_FWD_LAYERNORM_SM_MARGIN: 20 # TODO: set to 0 if not using 1F1B overlapping
  NVTE_BWD_LAYERNORM_SM_MARGIN: 20 # TODO: set to 0 if not using 1F1B overlapping
  NVTE_NORM_FWD_USE_CUDNN: 1
  NVTE_NORM_BWD_USE_CUDNN: 1
  PYTHONWARNINGS: ignore
  NCCL_DEBUG: VERSION

  # 128 Nodes w/ MTP w 1F1B overlapping
  # NOTE: currently, this default setting relies on the tuned-ckpt in CW cluster, it will OOM with the HF checkpoint
  # NOTE: with HF checkpoint, please put the MTP into the postprocess stage, please refer to the args below
  # MODEL=DeepSeek-V3 PP=8 VPP=4 TP=2 EP=64 NNODES=128 GBS=8192 PR=fp8 bash sbatch_benchmarking.sh

MODEL_ARGS:
  # Distributed args
  --distributed-timeout-minutes: 60
  --tensor-model-parallel-size: ${TP}
  --pipeline-model-parallel-size: ${PP}
  --expert-model-parallel-size: ${EP}
  --context-parallel-size: ${CP}
  --expert-tensor-parallel-size: 1
  --use-distributed-optimizer: true
  # TODO: uncomment if not using --delay-wgrad-compute
  # --overlap-grad-reduce: true
  # --overlap-param-gather: true

  # Training args
  --use-mcore-models: true
  --sequence-parallel: true
  --use-flash-attn: true
  --disable-bias-linear: true
  --micro-batch-size: ${MBS}
  --global-batch-size: ${GBS}
  --train-samples: 585937500
  --exit-duration-in-mins: 220
  --no-save-optim: true
  --no-check-for-nan-in-loss-and-grad: true
  --cross-entropy-loss-fusion: true
  --cross-entropy-fusion-impl: te # TODO: This is an EA feature, only available in te 2.2
  --manual-gc: true
  --manual-gc-interval: 10
  --disable-bf16-reduced-precision-matmul: true
  --recompute-granularity: selective
  --recompute-modules: mla_up_proj mlp

  # Transformer Engine args
  --transformer-impl: transformer_engine

  # Data args
  --seq-length: 4096
  --data-cache-path: ${WORKSPACE}/data_cache
  --tokenizer-type: HuggingFaceTokenizer
  --tokenizer-model: deepseek-ai/DeepSeek-V3
  --data-path: ${DATA_PATH}
  --split: 99,1,0
  --no-mmap-bin-files: true
  --no-create-attention-mask-in-dataloader: true
  --num-workers: 6

  # Add network size args
  --num-layers: 61
  --hidden-size: 7168
  --ffn-hidden-size: 18432
  --num-attention-heads: 128
  --kv-channels: 128
  --max-position-embeddings: 4096
  --position-embedding-type: rope
  --rotary-base: 10000
  --make-vocab-size-divisible-by: 3232
  --normalization: RMSNorm
  --norm-epsilon: 1e-6
  --swiglu: true
  --untie-embeddings-and-output-weights: true
  --multi-latent-attention: true

  # Add regularization args
  --attention-dropout: 0.0
  --hidden-dropout: 0.0
  --clip-grad: 1.0
  --weight-decay: 0.1
  --qk-layernorm: true

  # Add learning rate args
  --lr-decay-samples: 584765624
  --lr-warmup-samples: 1536000
  # Learning rate scaled down from 7.3e-6 (DeepSeek-V3 technical report, GBS=15360) to 3.9e-6 (GBS=8192)
  --lr-warmup-init: 3.9e-7
  --lr: 3.9e-6
  --min-lr: 3.9e-7
  --lr-decay-style: cosine
  --adam-beta1: 0.9
  --adam-beta2: 0.95

  # Add MoE args
  --num-experts: 256
  --moe-layer-freq: ([0]*3+[1]*58)
  --moe-ffn-hidden-size: 2048
  --moe-shared-expert-intermediate-size: 2048
  --moe-router-load-balancing-type: seq_aux_loss
  --moe-router-topk: 8
  --moe-token-dispatcher-type: flex
  --moe-enable-deepep: true
  --moe-router-pre-softmax: true
  --moe-grouped-gemm: true
  --moe-aux-loss-coeff: 1e-4
  --moe-router-group-topk: 4
  --moe-router-num-groups: 8
  --moe-router-topk-scaling-factor: 2.5
  --moe-router-score-function: sigmoid
  --moe-router-enable-expert-bias: true
  --moe-router-bias-update-rate: 1e-3
  --moe-router-dtype: fp32
  --moe-permute-fusion: true

  # Add MLA args
  --q-lora-rank: 1536
  --kv-lora-rank: 512
  --qk-head-dim: 128
  --qk-pos-emb-head-dim: 64
  --v-head-dim: 128
  --rotary-scaling-factor: 40
  --mscale: 1.0
  --mscale-all-dim: 1.0

  # EA: Add 1F1B overlapping
  --combined-1f1b: true
  --delay-wgrad-compute: true
  # EA: Add MTP and parallel layout args
  # enable MTP with 1F1B overlapping, need the tuned-ckpt in CW cluster
  --pipeline-model-parallel-layout: Et*3|(tt|)*29m|L
  --mtp-num-layers: 1
  --mtp-loss-scaling-factor: 0.1
  --mtp-1f1b-overlap: true
  # !!!!!!!!!!!! use below settings with HF checkpoint !!!!!!!!!!!!
  # # put MTP into the postprocess stage
  # --pipeline-model-parallel-layout: Et|(tt|)*30mL
  # --mtp-num-layers: 1
  # --mtp-loss-scaling-factor: 0.1
  # # disable MTP
  # --pipeline-model-parallel-layout: Et|(tt|)*30L

  # Add validation args
  --eval-iters: 32
  --eval-interval: 200

  # Add checkpointing args
  --finetune: false
  --no-load-optim: true
  --no-load-rng: true
  --auto-detect-ckpt-format: true
  --load: ${LOAD_PATH}
  --save: ${OUTPUT_PATH}/checkpoints
  --save-interval: 500
  --dist-ckpt-strictness: log_all

  # Add initialization args
  --init-method-std: 0.02

  # Add logging args
  --log-timers-to-tensorboard: false
  --log-memory-to-tensorboard: true
  --log-num-zeros-in-grad: false
  --log-params-norm: false
  --log-validation-ppl-to-tensorboard: true
  --log-throughput: true
  --log-interval: 1
  --logging-level: 40
  --tensorboard-dir: ${OUTPUT_PATH}/tensorboard
  --wandb-project: ${WANDB_PROJECT}
  --wandb-exp-name: DeepSeek-V3-TP${TP}PP${PP}EP${EP}CP${CP}VPP${VPP}-MBS${MBS}GBS${GBS}-${COMMENT}

  # Add mixed precision args
  --bf16: true
